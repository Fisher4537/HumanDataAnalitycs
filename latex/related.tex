% !TEX root = CartaPerali_Report.tex

\section{Related Work}
\label{sec:related_work}

\noindent 
\mbox{Deep Neural Networks} are the most powerful tool for the KWS task as they allow to reach state-of-art performances. \cite {Jansson-2018} While traditional machine learning techniques e.g. \mbox{Support Vectors Machines} (SVM), require hand-made features to reach optimal results, Neural Networks are capable of successfully train from more raw data. \cite{Tang-2018} Their deployment of course is not new. CNN are the standard tool for small-footprint KWS since they have a relatively standard architecture which is easy to implement and tune. Additionally, they are available in several of the most used \mbox{Deep Learning} frameworks. \cite{Andrade-2018} introduces a novel attention-based \mbox{Recurrent Neural Networks} (RNN) architecture designed to recognize simple speech commands, while still generating a lightweight model that can be loaded in mobile devices and run locally. In \cite{Zang-2018} the \mbox{Key-word/Garbage} model is used. A \mbox{Hidden Markov Model} (HMM) is constructed for each keyword. In addition, garbage HMMs are constructed to represent all non-keywords. A Viterbi decoding is then used to find the best path that explains the input speech. Obtained results show very low false reject rates but, very high false alarm rates, since the garbage model can represents all non-keywords properly.