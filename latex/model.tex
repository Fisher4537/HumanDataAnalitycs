% !TEX root = CartaPerali_Report.tex
\section{Processing Pipeline}
\label{sec:processing_pipeine}

\noindent The code necessary to replicate this work is available in the GitHub repository \footnote{https://github.com/Fisher4537/HumanDataAnalitycs}.\\
The workflow used to implement the whole KWS system is depicted in Fig. \ref{fig:pipeline}.
\begin{figure}[h]
			\centering
	    	\includegraphics[width=6cm, height=8cm ,width=0.25\textwidth]{pipeline}
	    	\caption{System workflow}
	    	\label{fig:pipeline}
\end{figure} 
\noindent As mentioned in the intro, the input data are downloaded from the well known public '{\it{speech\_commands V2}}' dataset \cite{Warden-2018}. At the time of this project it counts 105829 audio file, in \textit{.wav} format, 16000 Hz, channel mono with the duration of one second each. Each of them is listed and the file dimension is checked: only the file with size of 32K are kept, the other are discarded, indeed the audio with length less than 1 s have an higher probability to contain truncated word spell. \\
As suggested by the author of the dataset \cite{Warden-2018}, to split the it into \textit{train}, \textit{validation} and \textit{test}, the hash of each audio is used to avoid that two different audio with the same label and pronounced by the same person would be put on different set (see \textit{which\_set} method described by the author in the README), compromising the interdependence between the different sets. \\
The dataset is too huge be loaded into conventional RAM, therefore each list (train, validation and test) is dynamically split into lighter batches by specific generator (represented as dashed line in fig. \ref{fig:pipeline}). Each generator return a preprocessed audio as described next, which is consumed during the train and/or test phase.\\
\textbf{Balanced batches and unknown percentage.} In a real application an audio stream is full of background noise and utterance to ignore. The keywords to recognize are only a little part. To differentiate background noise and unknown utterance, an \textit{unknown} label is added to the wanted words. The \textit{unknown} class should be composed of audio depending of the application context. In this report the problem is not faced. However it is provided a simple mechanism for the management of unknown label and the percentage of each label of the dataset. A list of \textit{wanted words} is passed as input parameter with an unknown percentage value. Each returned batch has the specified percentage of unknown audios taken from the file which are not in the wanted word list. In the report, each dataset with a specific percentage of unknown is defined as \textit{balanced}, instead, \textit{unbalanced} dataset has no specific unknown percentage and that class composed of all the speech recognition audio that are not labeled with a specified wanted word.


\section{Data preprocessing and Features extraction}
\label{sec:model}

\noindent Data are preprocessed by \textit{generators} that are responsible for preparing batches of data containing the desired percentage of predefined wanted words, organized in a list and the percentage of words that the model should classify as \textit{unknown} if used.\\
The preprocessed example are obtained from the raw signal of the "bad" utterance represented in figure \ref{fig:bed_rawsignal}.\\

\noindent \textbf{MFCC.} The raw data is normalized from -1 to 1 and the Mel-Frequency Cepstrum Coefficients are computed with \textit{python\_speech\_features} library. This one apply the Fast Fourier Transform, the power of the spectrum obtained is mapped onto the mel scale using triangular overlapping, then the logs of the powers at each of the mel frequencies is taken and in the end the discrete cosine transform of the list of mel log powers is computed. MFCC frames are built using the selected parameters depicted in Table \ref{table:mfcc_parameters}.
\noindent In the end, one set of 13 MFCC coefficients is extracted from each frame. In figure \ref{fig:bed_mfcc} is shown an example of preprocess of an input signal.
\begin{table}[h]
	\centering
	\begin{tabular}{ lr}
		Window length & 25 ms        \\
		Window step & 0.01 ms        \\
		Frame shift  & 10 ms         \\
		N. of cepstrum to return & 13 \\
		N. Filterbanks & 26         \\
		N. FFT points & 512         \\
		\hline
	\end{tabular}
	\caption{MFCC parameters}
	\label{table:mfcc_parameters} 
\end{table}\\
\noindent \textbf{Spectrogram.} Raw data are normalized from 1 to -1, split into segments, multiply by the windowing function and then the Fast Fourier Transform is apply.
The parameter values used to compute the spectrogram are in Table \ref{table:specgram_parameters}. In figure \ref{fig:bed_specgram} is shown an example of preprocess of an input signal.
\begin{table}[h]
	\centering
	\begin{tabular}{ lr}
		N. of points per segment & 256 \\
		Windowing function & Hanning window \\
		N. of overlapping points between segments & 128 \\
		\hline
	\end{tabular}
	\caption{Spectrogram parameters}
	\label{table:specgram_parameters} 
\end{table}\\

\begin{figure}
	\centering
	\includegraphics[width=.5\textwidth]{img/bed_rawsignal_plot.png}
	\caption{Raw signal of the \textit{bad} utterance}
	\label{fig:bed_rawsignal}
\end{figure}


\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{img/bed_mfcc_plot.png}
	\caption{MFCC of the \textit{bad} utterance}
	\label{fig:bed_mfcc}
\end{figure}


\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{img/bed_specgram_matplotlib.png}
	\caption{Spectrogram of the \textit{bad} utterance}
	\label{fig:bed_specgram}
\end{figure}


\section{Learning Framework}
\label{sec:learning_framework}

\subsection*{\textbf {CNN architectures and training}}Processed input data contain at this point the feature vectors ready to fed as input to the model. Different model structure has been analyzed using the convolutional layer as kernel of the neural network. Here there is a description of the implemented models:
\begin{itemize}
	\item \textbf{Light CNN} (light\_cnn): This is the simplest proposed model and is shown in figure \ref{fig:CNN_schema}.
	\item \textbf{Light CNN with regularization} (light\_cnn\_reg\_2): an alternative of the first model with Regularization L1\_L2 in of the weight of the first Convolutional layer.
	\item \textbf{Light CNN with Regularization and Dropout} (light\_cnn\_reg\_drop): an extension with a Dropout layer after the first Convolutional layer.
	\item \textbf{Max pool} (mp): This is an alternative of light\_cnn with a MaxPool2D after each Convolutional layer.
	\item \textbf{Double Dence} (dd): the light\_cnn with two Dence layer at the end of the sequential model.
	\item \textbf{Double Dence with Dropout} (dd\_drop): an specialized version of \textit{dd} with a Dropout layer after the first Dense layer.
\end{itemize}
\begin{figure}[h]
	\centering
	\includegraphics[width=5, height=8cm, width=0.25\textwidth]{CNN_schema}
	\caption{Light\_cnn model}
	\label{fig:CNN_schema}
\end{figure} 
\textbf{Optimizer.} The models are trained and optimized with two different built-in optimization algorithms in order to subsequently compare the obtained results. The deployed optimizers are the classic Stochastic Gradient Descent (SGD) and the Adam algorithm. \\
\textbf{Callbacks.} During the training of the model, several callbacks are passed as input to improve the performances in a computationally smart way. The {\it{ReduceOnPlateau}} callback takes care of setting the learning rate according to the behavior of the accuracy's curve; the {\it{EarlyStopping}} callback has the purpose to stop the training process, if no significant improvement is observed for a defined number of epochs. The {\it{TensorBoard}} callback function provides an interface to show the behavior of the metrics of interest during the training.

\section{Training analysis}
In Fig. \ref{fig:CNN_loss} the behavior of the loss during the training phase is shown and in Fig.  \ref{fig:CNN_accuracy} accuracy's one is depicted.

\begin{figure}[h]
			\centering
	    	\includegraphics[width=8cm, height=6cm]{debug_tuy_acc}
	    	\caption{CNN training accuracy}
	    	\label{fig:CNN_accuracy}
\end{figure} 

\begin{figure}[h]
			\centering
	    	\includegraphics[width=8cm, height=6cm]{debug_tuy_loss}
	    	\caption{CNN training loss}
	    	\label{fig:CNN_loss}
\end{figure} 



